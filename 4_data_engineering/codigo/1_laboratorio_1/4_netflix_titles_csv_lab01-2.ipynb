{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idUGBPnUx4PH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zona horaria de Perú\n",
        "os.environ['TZ'] = 'America/Lima'\n",
        "time.tzset()  # Solo funciona en sistemas tipo Unix (como Colab o Linux)"
      ],
      "metadata": {
        "id": "b-U671azRUdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset y configurar logging para que funcione en Colab / Jupyter\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")"
      ],
      "metadata": {
        "id": "xqpZfVuQPXHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes\n",
        "GITHUB_ROOT_PATH = 'https://github.com/AngelTintaya/datasets/raw/refs/heads/main'\n",
        "ROOT_DIR = '/content/drive/MyDrive/Datasets/datalake'\n",
        "OUTPUT_DIR = os.path.join(ROOT_DIR, 'datalake_outputs')"
      ],
      "metadata": {
        "id": "XL-Qi5vbGhIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config_file(file_name: str) -> dict[str:str]:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    GITHUB_FILE_PATH = os.path.join(GITHUB_ROOT_PATH, file_name) #(1) Ruta del archivo en github\n",
        "    OUTPUT_FILE_PATH = os.path.join(OUTPUT_DIR, file_name) #(2)\n",
        "\n",
        "    return {\n",
        "        'GITHUB_FILE': file_name,\n",
        "        'GITHUB_FILE_PATH': GITHUB_FILE_PATH,\n",
        "        'OUTPUT_FILE_PATH': OUTPUT_FILE_PATH\n",
        "    }"
      ],
      "metadata": {
        "id": "rUo88QNMBduF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validar_existencia_archivo(path: str) -> bool:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    response = requests.head(path, allow_redirects=True) #(3) revisa existencia del archivo en github\n",
        "    if response.status_code != 200:\n",
        "        logging.warning(f\"El archivo no existe en el repositorio: {path}\") #(4)\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "fBpbwu4IC5hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_directorio(path: str) -> None:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path): #(5)\n",
        "        os.makedirs(path) #(6)\n",
        "        return\n",
        "\n",
        "    for item in os.listdir(path): #(7)\n",
        "        ruta = os.path.join(path, item) #(8)\n",
        "        if os.path.isfile(ruta): #(9)\n",
        "            os.remove(ruta) #(10)\n",
        "        elif os.path.isdir(ruta): #(11)\n",
        "            shutil.rmtree(ruta) #(12)\n",
        "    logging.info(f\"Directorio limpiado: {path}\")"
      ],
      "metadata": {
        "id": "AojooPkLR9v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leer_y_guardar_csv_en_datalake(file_path: str, output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_csv = pd.read_csv(file_path) #(13)\n",
        "        df_csv.to_csv(output_path, index=False) #(14)\n",
        "        logging.info(f\"Guardado exitosamente: {output_path}\")\n",
        "    except Exception as e: #(15)\n",
        "        logging.error(f\"Error al procesar archivo {file_path}: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "c9pRygCXyWvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_archivo_desde_github(file_name: str = 'dummy.csv') -> None:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    logging.info(f\"{'='*20} PROCESANDO: {file_name} {'='*20}\")\n",
        "\n",
        "    config_file = get_config_file(file_name) #(16)\n",
        "    GITHUB_FILE_PATH = config_file['GITHUB_FILE_PATH'] #(17)\n",
        "    OUTPUT_FILE_PATH = config_file['OUTPUT_FILE_PATH'] #(18)\n",
        "\n",
        "    # Crear directorio si no existe\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True) #(19)\n",
        "\n",
        "    if not validar_existencia_archivo(GITHUB_FILE_PATH):\n",
        "        logging.warning(f\"Archivo no encontrado: {GITHUB_FILE_PATH}\")\n",
        "        return\n",
        "\n",
        "    leer_y_guardar_csv_en_datalake(GITHUB_FILE_PATH, OUTPUT_FILE_PATH) #(20)\n",
        "\n",
        "    logging.info(f\"{'='*20} FINALIZADO: {file_name} {'='*20}\")"
      ],
      "metadata": {
        "id": "V_OJyW1z8a2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    limpiar_directorio(OUTPUT_DIR) #(21)\n",
        "    time.sleep(5)  # Solo para demostración visual, puede eliminarse en producción\n",
        "\n",
        "    archivos = [\n",
        "        'netflix_titles.csv',\n",
        "        'netflix_titles_directors.csv',\n",
        "        'netflix_titles_countries.csv',\n",
        "        'netflix_titles_cast.csv',\n",
        "        'netflix_titles_category.csv'\n",
        "    ] #(22)\n",
        "\n",
        "    for archivo in archivos: #(23)\n",
        "        procesar_archivo_desde_github(archivo) #(24)"
      ],
      "metadata": {
        "id": "HvmX-7yBE36s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sólo se ejecuta si el archivo se ejecuta directamente\n",
        "if __name__ == \"__main__\": #(25)\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1QU-EfKyWsb",
        "outputId": "62e56653-b4ff-41a0-bb13-5a11a79b4625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-19 19:23:58,590 - INFO - Directorio limpiado: /content/drive/MyDrive/Datasets/datalake/datalake_outputs\n",
            "2025-09-19 19:24:03,592 - INFO - ==================== PROCESANDO: netflix_titles.csv ====================\n",
            "2025-09-19 19:24:05,145 - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles.csv\n",
            "2025-09-19 19:24:05,149 - INFO - ==================== FINALIZADO: netflix_titles.csv ====================\n",
            "2025-09-19 19:24:05,150 - INFO - ==================== PROCESANDO: netflix_titles_directors.csv ====================\n",
            "2025-09-19 19:24:05,731 - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_directors.csv\n",
            "2025-09-19 19:24:05,735 - INFO - ==================== FINALIZADO: netflix_titles_directors.csv ====================\n",
            "2025-09-19 19:24:05,737 - INFO - ==================== PROCESANDO: netflix_titles_countries.csv ====================\n",
            "2025-09-19 19:24:06,250 - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_countries.csv\n",
            "2025-09-19 19:24:06,251 - INFO - ==================== FINALIZADO: netflix_titles_countries.csv ====================\n",
            "2025-09-19 19:24:06,253 - INFO - ==================== PROCESANDO: netflix_titles_cast.csv ====================\n",
            "2025-09-19 19:24:07,393 - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_cast.csv\n",
            "2025-09-19 19:24:07,396 - INFO - ==================== FINALIZADO: netflix_titles_cast.csv ====================\n",
            "2025-09-19 19:24:07,397 - INFO - ==================== PROCESANDO: netflix_titles_category.csv ====================\n",
            "2025-09-19 19:24:07,962 - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_category.csv\n",
            "2025-09-19 19:24:07,964 - INFO - ==================== FINALIZADO: netflix_titles_category.csv ====================\n"
          ]
        }
      ]
    }
  ]
}