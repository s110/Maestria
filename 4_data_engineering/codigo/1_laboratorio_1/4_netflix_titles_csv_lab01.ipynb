{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "idUGBPnUx4PH",
    "ExecuteTime": {
     "end_time": "2025-09-27T05:35:24.051933Z",
     "start_time": "2025-09-27T05:35:23.620534Z"
    }
   },
   "source": [
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import shutil"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Zona horaria de Perú\n",
    "os.environ['TZ'] = 'America/Lima'\n",
    "time.tzset()  # Solo funciona en sistemas tipo Unix (como Colab o Linux)"
   ],
   "metadata": {
    "id": "b-U671azRUdp",
    "ExecuteTime": {
     "end_time": "2025-09-27T05:35:24.067355Z",
     "start_time": "2025-09-27T05:35:24.064732Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Logging setup ---\n",
    "# for handler in logging.root.handlers[:]:\n",
    "#     logging.root.removeHandler(handler)\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN DE LOGGER\n",
    "# =========================\n",
    "logger = logging.getLogger(\"ETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Evitar duplicados en Colab / Jupyter\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ],
   "metadata": {
    "id": "KMKZ_W3YETxZ",
    "ExecuteTime": {
     "end_time": "2025-09-27T05:35:24.123541Z",
     "start_time": "2025-09-27T05:35:24.119566Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# Constantes\n",
    "# GITHUB_ROOT_PATH = 'https://github.com/AngelTintaya/datasets/raw/refs/heads/main'\n",
    "GITHUB_ROOT_PATH = 'https://raw.githubusercontent.com/AngelTintaya/datasets/refs/heads/main'\n",
    "ROOT_DIR = '/content/drive/MyDrive/Datasets/datalake'\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'datalake_outputs')"
   ],
   "metadata": {
    "id": "XL-Qi5vbGhIr",
    "ExecuteTime": {
     "end_time": "2025-09-27T05:46:14.459683Z",
     "start_time": "2025-09-27T05:46:14.456981Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def get_config_file(file_name: str) -> dict[str:str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    GITHUB_FILE_PATH = os.path.join(GITHUB_ROOT_PATH, file_name) #(1) Ruta del archivo en github\n",
    "    OUTPUT_FILE_PATH = os.path.join(OUTPUT_DIR, file_name) #(2)\n",
    "\n",
    "    return {\n",
    "        'GITHUB_FILE': file_name,\n",
    "        'GITHUB_FILE_PATH': GITHUB_FILE_PATH,\n",
    "        'OUTPUT_FILE_PATH': OUTPUT_FILE_PATH\n",
    "    }"
   ],
   "metadata": {
    "id": "rUo88QNMBduF"
   },
   "execution_count": 128,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def validar_existencia_archivo(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    response = requests.head(path, allow_redirects=True) #(3) revisa existencia del archivo en github\n",
    "    if response.status_code != 200:\n",
    "        logger.warning(f\"El archivo no existe en el repositorio: {path}\") #(4)\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "id": "fBpbwu4IC5hD"
   },
   "execution_count": 131,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def limpiar_directorio(path: str) -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path): #(5)\n",
    "        os.makedirs(path) #(6)\n",
    "        return\n",
    "\n",
    "    for item in os.listdir(path): #(7)\n",
    "        ruta = os.path.join(path, item) #(8)\n",
    "        if os.path.isfile(ruta): #(9)\n",
    "            os.remove(ruta) #(10)\n",
    "        elif os.path.isdir(ruta): #(11)\n",
    "            shutil.rmtree(ruta) #(12)\n",
    "    logger.info(f\"Directorio limpiado: {path}\")"
   ],
   "metadata": {
    "id": "AojooPkLR9v9"
   },
   "execution_count": 135,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def leer_y_guardar_csv_en_datalake(file_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_csv = pd.read_csv(file_path) #(13)\n",
    "        df_csv.to_csv(output_path, index=False) #(14)\n",
    "        logger.info(f\"Guardado exitosamente: {output_path}\")\n",
    "    except Exception as e: #(15)\n",
    "        logger.error(f\"Error al procesar archivo {file_path}: {e}\")\n",
    "        raise"
   ],
   "metadata": {
    "id": "c9pRygCXyWvT"
   },
   "execution_count": 136,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def procesar_archivo_desde_github(file_name: str = 'dummy.csv') -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    logger.info(f\"{'='*20} PROCESANDO: {file_name} {'='*20}\")\n",
    "\n",
    "    config_file = get_config_file(file_name) #(16)\n",
    "    GITHUB_FILE_PATH = config_file['GITHUB_FILE_PATH'] #(17)\n",
    "    OUTPUT_FILE_PATH = config_file['OUTPUT_FILE_PATH'] #(18)\n",
    "\n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True) #(19)\n",
    "\n",
    "    if not validar_existencia_archivo(GITHUB_FILE_PATH):\n",
    "        logger.warning(f\"Archivo no encontrado: {GITHUB_FILE_PATH}\")\n",
    "        return\n",
    "\n",
    "    leer_y_guardar_csv_en_datalake(GITHUB_FILE_PATH, OUTPUT_FILE_PATH) #(20)\n",
    "\n",
    "    logger.info(f\"{'='*20} FINALIZADO: {file_name} {'='*20}\")"
   ],
   "metadata": {
    "id": "V_OJyW1z8a2-"
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    logger.debug(\"==> Ejecutando ETL\")\n",
    "    limpiar_directorio(OUTPUT_DIR) #(21)\n",
    "    logger.debug(\"==> Validar archivos han sido borrados\")\n",
    "    time.sleep(5)  # Solo para demostración visual, puede eliminarse en producción\n",
    "\n",
    "    archivos = [\n",
    "        'netflix_titles.csv',\n",
    "        'netflix_titles_directors.csv',\n",
    "        'netflix_titles_countries.csv',\n",
    "        'netflix_titles_cast.csv',\n",
    "        'netflix_titles_category.csv'\n",
    "    ] #(22)\n",
    "\n",
    "    for archivo in archivos: #(23)\n",
    "        procesar_archivo_desde_github(archivo) #(24)\n",
    "\n",
    "    logger.debug(\"==> Terminando ETL\")"
   ],
   "metadata": {
    "id": "HvmX-7yBE36s"
   },
   "execution_count": 140,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sólo se ejecuta si el archivo se ejecuta directamente\n",
    "if __name__ == \"__main__\": #(25)\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1QU-EfKyWsb",
    "outputId": "98fc9d8c-da41-4646-e182-98e34c665e6e"
   },
   "execution_count": 141,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2025-09-20 10:04:54,773 - ETL - INFO - Directorio limpiado: /content/drive/MyDrive/Datasets/datalake/datalake_outputs\n",
      "2025-09-20 10:04:59,775 - ETL - INFO - ==================== PROCESANDO: netflix_titles.csv ====================\n",
      "2025-09-20 10:05:00,434 - ETL - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles.csv\n",
      "2025-09-20 10:05:00,437 - ETL - INFO - ==================== FINALIZADO: netflix_titles.csv ====================\n",
      "2025-09-20 10:05:00,438 - ETL - INFO - ==================== PROCESANDO: netflix_titles_directors.csv ====================\n",
      "2025-09-20 10:05:00,716 - ETL - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_directors.csv\n",
      "2025-09-20 10:05:00,718 - ETL - INFO - ==================== FINALIZADO: netflix_titles_directors.csv ====================\n",
      "2025-09-20 10:05:00,720 - ETL - INFO - ==================== PROCESANDO: netflix_titles_countries.csv ====================\n",
      "2025-09-20 10:05:01,097 - ETL - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_countries.csv\n",
      "2025-09-20 10:05:01,100 - ETL - INFO - ==================== FINALIZADO: netflix_titles_countries.csv ====================\n",
      "2025-09-20 10:05:01,101 - ETL - INFO - ==================== PROCESANDO: netflix_titles_cast.csv ====================\n",
      "2025-09-20 10:05:01,663 - ETL - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_cast.csv\n",
      "2025-09-20 10:05:01,668 - ETL - INFO - ==================== FINALIZADO: netflix_titles_cast.csv ====================\n",
      "2025-09-20 10:05:01,668 - ETL - INFO - ==================== PROCESANDO: netflix_titles_category.csv ====================\n",
      "2025-09-20 10:05:02,013 - ETL - INFO - Guardado exitosamente: /content/drive/MyDrive/Datasets/datalake/datalake_outputs/netflix_titles_category.csv\n",
      "2025-09-20 10:05:02,016 - ETL - INFO - ==================== FINALIZADO: netflix_titles_category.csv ====================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ],
   "metadata": {
    "id": "raC1KKF8G30l"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
